\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%Enélkül rossz a referencia szamozas!!!!!
\PassOptionsToPackage{square,sort,comma,numbers}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

%\usepackage{nips_2016}
\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}	%inserting images
\usepackage{hyperref}
\usepackage{float}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{listings}


\DeclareCaptionSubType * [alph]{table}
\captionsetup[subtable]{labelformat=simple, labelsep=space}

\renewcommand\thesubtable{\thetable(\alph{subtable})}

%magyar kivonathoz
\usepackage[magyar, english]{babel}

\title{Reinforcement learning based self-driving vehicles in the Duckietown environment\\
	\large Megerősítéses tanulás alapú önvezető járművek a Duckietown környezetben}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Attila Pethő\\\\
  %\texttt{attilapetho13@gmail.com} \\
  %% examples of more authors
   \And  
   Olivér István Farkas\\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
   \And
    Gyula Faragó\\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

%\selectlanguage{magyar}
%\begin{abstract}
%Kivonat magyarul
%\end{abstract}

%\selectlanguage{english}

\begin{center}
	\large
	\textbf{Abstract}\\
\end{center}
In this project we tried to bring a solution to driving a small robot with vision-based, end-to-end reinforcement learning in a simulated environment. The robot relies on images from its front-facing camera. Our models generate continous actions that control a differential-drive robot vehicle. To train the policy we have selected two state-of-the-art algorithms (A2C and PPO). A2C is an Actor Critic model, while PPO uses Proximal Policy Optimization. 
In order to train these models we have to feed in the images, which we preprocess first. To train the model better for real-world conditions we used domain randomization. Further analysis of our work is discussed in the document.

\begin{center}
	\large
	\textbf{Kivonat}\\
\end{center}

Ebben a projektben megpróbáltunk megoldást találni egy kis robot vezetésére látásalapú megerősített tanulással egy szimulált környezetben. A robot az elöl elhelyezkedő kamerájának képeire támaszkodik. Modelljeink folyamatos cselekvéseket generálnak, amelyek egy differenciálhajtású robotjárművet irányítanak. A stratégia betanításához két modern algoritmust választottunk (A2C és PPO). 
Az A2C egy Actor Critic modell, míg a PPO a Proximal Policy Optimizationt használja. A modellek a környezettől kapott képkockák alapján tanulnak, amelyeket először előfeldolgozunk. Ahhoz, hogy a modell valós környezetbe átültetését elősegítsük, domain randomizálást használtunk. Munkánk további elemzését a dokumentumban tárgyaljuk.

\section{\large{Introduction}}

Self-driving vehicles are clearly the future of transportation. If you look at Waymo's \cite{waymo}  safety report you can see two things. First, it is obvious that AI driven vehicles are more safe than human driven ones. The other thing - which makes development hard - is the amount of data needed for the training of the AI. 
The report mentions that the AI used in the cars have learned from over 20 million driven miles. It is quite a big number and an astounding achievement given the fact that the car fleet only had 8 bigger accident during this time. The report shows that all accident had a similar cause, human error. In three times the car was stationary and someone ran into it. This clearly shows the potential of self driving vehicles.

In the field of self-driving cars, reinforcement learning is a very promising technique. Training self-driving agents in an end-to-end fashion is getting more and more attention among researchers. For example Minh et al. \cite{DBLP:journals/corr/MnihBMGLHSK16} proposed a vehicle-controller policy, that operates with discrete actions, based on a forward-facing camera on the robot. Although Kendall et al. \cite{kendall2018learning} demonstrated real-world driving with a lane-following policy on a real vehicle, a safety driver was needed for supervision. 
Training models on real vehicles raises both safety and financial concerns, so a better solution is to experiment in a simulator. One example is the work of Jaritz et al. \cite{DBLP:journals/corr/abs-1807-02371}, who used the World Rally Championship 6 racing simulator to train their vision-based road-following policy. Balaji et al. \cite{DBLP:journals/corr/abs-1911-01562} aslo presented a method for training vehicle-controller policies in a simulator, but their primary platform was the DeepRacer platform. Our choice was the Duckietown simulator, which is a fully-functioning autonomous driving simulator that can be used to train and test different vehicle-controller algorithms \cite{gymdt}.

\section{\large{Methods}}

\subsection{\normalsize{Reinforcement Learning}}

In a reinforcement learning setting, an agent interacts with its environment by choosing actions (a\textsubscript{t}) at every timestep, based on information called state (s\textsubscript{t}), and receives back the next state (s\textsubscript{t+1}) and the reward for the current action (r\textsubscript{t+1}), which will be used to select the next action (a\textsubscript{t+1}) \cite{dlr_book}. \autoref{fig:statemachin} summarizes this process. 
The agent tries to gradually improve its policy by exploring new ways to behave in the environment and observing the rewards received for its actions. The goal of the agent is to maximize the expected future rewards over a finite amount of steps. 
There are many ways of achieving this goal, which is reflected in the number of different reinforcement learning algorithms. A fairly simple one is Deep Q-Networks (DQN) \cite{DBLP:journals/corr/MnihKSGAWR13} which chooses actions based on an estimate of the action-value function that is calculated from previous steps in the environment. 
Although it has been used in a similar setting \cite{DBLP:journals/corr/abs-2009-11212}, DQN would not be the best choice for this type of problem, because it can only utilize discrete actions, which could potentially cause wobbling and unstable behaviour \cite{DBLP:journals/corr/abs-1807-02371}, \cite{DBLP:journals/corr/abs-1911-12905}. For this reason, we chose two algorithms, Proximal Policy Optimization (PPO) \cite{DBLP:journals/corr/SchulmanWDRK17} and a synchronous, deterministic variant of Asynchronous Advantage Actor Critic (A2C) \cite{DBLP:journals/corr/MnihBMGLHSK16}, which both support continuous actions.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{rl.jpg}
	\caption{Illustration of Reinforcement Learning}
	\label{fig:statemachin}
\end{figure}

Implementing the above mentioned algorithms would be very time consuming, so we decided to use the Stable Baselines 3 library \cite{stablebase}, which is a simplified version of OpenAI baselines. The SB3 library contains several implementations of state-of-the-art RL algorithms and useful tools to train, retrain and evaluate the models.

\subsection{\normalsize{Environment}}

In each step, the environment gives us a 640x480 RGB image.
We have to preprocess this image, because feeding the original images to the CNN would be a waste of resources because it makes the training process much slower, and the network can learn from smaller images just as well.

\subsubsection{\normalsize{Observations}}

Preprocessing steps are applied to the observations. These included resizing the image from 640x480 pixels to 84x84, cropping the top 24 pixels of the image (since this only shows the sky), grayscaling the image, or optionally segment the colors in it, and finally stack the last 3 frames. The preprocessing steps were implemented using wrappers, that were applied to the environment layer-by-layer.

\begin{figure}[H]
	\centering
	\begin{minipage}{.3\textwidth}
	\centering
	\includegraphics[width=0.8\linewidth]{rawobs.jpg}
	\caption{(a)}
	\end{minipage}%
	\begin{minipage}{.3\textwidth}
	\centering
	\includegraphics[width=0.8\linewidth]{grayscale.jpg}
	\caption{(b)}
	\end{minipage}%
\begin{minipage}{.3\textwidth}
	\centering
	\includegraphics[width=0.8\linewidth]{colorseg.jpg}
	\caption{(c)}
\end{minipage}
\caption{Observations from the environment. (a) shows a raw observation, (b) shows a grayscaled image, (c) shows a color segmentated image}
\end{figure}

\subsubsection{\normalsize{Actions}}
\label{sec:actions}
The Duckiebots are two-wheel differential-drive vehicles, and the default actions in the simulator are interpreted as a list of the left and the right wheel angular velocities, where 1 and -1 correspond to the maximum angular velocities forward and backward. This continuous action space has left and right velocity pairs that are not particularly useful (e.g. low velocities), so we experimented with two new action representations, introduced by Kalapos et al. \cite{kalap}.

\textbf{Wheel velocity with braking:} Since negative velocities are not required for moving forward, these are clipped, and the predicted values are interpreted as the amount of braking from full speed at each wheel. This creates a bias for moving forward at full speed.

\textbf{Steering:} Instead of predicting a vector containing the two wheel velocities, the agent predicts a scalar value between -1 and 1, where 0.0 represents going forward at full speed, and -1 and 1 correspond to turning left or right with one wheel completely stopped and the other spinning at maximum speed. Intermediate actions are continuously mapped to wheel velocities using linear interpolation. In this case the speed is always maximal for a given steering value.

\subsubsection{\normalsize{Rewards}}
\label{sec:rewards}
Reward shaping is essential in RL, since this is the basis for the agent's learning. Without a good reward function the agent will maximize its score, but won't learn the intended behaviour. The goal of the robot should be to move forward in the right lane as fast as possible. To encourage this behaviour we used a reward fuction \cite{kalap} that considers both the orientation and the velocity of the Duckiebot. The \textit{orientation} reward is based on the lateral position of the robot, so it should always face towards the center of the right lane, while the \textit{velocity} reward is given based on the fastest moving wheel to encourage high velocity in both straight and curved sections.

\subsection{\normalsize{Evaluation}}
There are many ways to measure the performance of the trained agents, and for the sake of simplicity we chose two metrics for this purpose: survival time (in timesteps) and rewards received from the environment. Survival time tells us how many steps the agent took before either it went out of the road or the episode finished. A well-trained agent should stay on the road for the whole evaluation episode. 
Altough, this provides valuable information, this metric does not tell much about performance on its own. Hence, the rewards are also stored and evaluated to gain richer knowledge about the agents perfomance. The models were trained on the \textit{'zigzag\_dists'} map, because it offers a good balance between left and right turns and straight sections. In order to test how well the models learned to generalize, evaluation was done on three different maps: \textit{'zigzag\_dists'}, \textit{'loop\_empty'}, and \textit{'udem1'}.

\subsection{\normalsize{Hyperparameter Optimization}}
Reinforcement learning models are extremely sensitive to hyperparameter settings. A poor choice of hyperparameters can cause unstable performance or it could prevent convergence completely. Findig the right parameters for training can be a difficult job, and it is usually not a good idea to start tuning the parameters manually. Instead you can use an optimization library, that sets up sophisticated experiments to find the right hyperparameters for your usecase.

For our project we used the Optuna library, which offers several tools to dynamically explore the hyperparameter-space in a define-by-run style. The library features state-of-the-art alorithms for sampling and pruning, easy parallelization of experiments and visualization tools. In order to optimize our models, we used the Tree-structured Parzen Estimator (TPE) for sampling and the Median pruner for early termination of less promising trials. 
The studies were conducted with a budget of 100 trials and a maximum of 50000 steps, and evaluations at every 10000 steps. For optimizating the PPO model, we used an RTX 6000 workstation with 24GB VRAM and 46GB RAM, but for the A2C study, we had to use a GTX 1060OC with 6GB of VRAM and 16GB RAM which was not enough to finish the study, so in that case the results are based on 47 finished trials. Tables \ref{tab:ppo} and \ref{tab:a2c} shows the optimized hyperparameters for the models.




\begin{table}[H]
	\begin{subtable}[h]{0.45\textwidth}
		\centering
		\begin{tabular}{|l | l|}
		\hline
		Learning rate schedule & constant  \\
		\hline
		Learning rate & 0.1324  \\
		\hline
		Norm. advantage & False  \\
		\hline
		n\_steps & 64  \\
		\hline
		Entropy coef. & 0.0015 \\
		\hline
		Use RMS prop. & False  \\
		\hline
		GAE. lambda & 0.98 \\
		\hline
		Max gradient norm. & 0.7  \\
		\hline
		Value f. coef. & 0.7373  \\
		\hline
		Activation  & tanh  \\
		\hline
		\end{tabular}
		\caption{A2C}
		\label{tab:a2c}
	\end{subtable}
	\hfill
	\begin{subtable}[h]{0.45\textwidth}
		\centering
		\begin{tabular}{|l | l |}
		\hline
		Learning rate schedule & linear \\
		\hline 
		Learning rate & 0.8764  \\
		\hline
		Batch size & 256  \\
		\hline
		n\_steps & 32  \\
		\hline
		Entropy coef. & 6.5699e-5 \\
		\hline
		Clip range & 0.2  \\
		\hline
		Epochs & 5 \\
		\hline
		GAE. lambda & 0.92 \\
		\hline
		Max gradient norm. &  0.8 \\
		\hline
		Value f. coef. & 0.2425  \\
		\hline
		Activation & relu \\
		\hline
		\end{tabular}
		\caption{PPO}
		\label{tab:ppo}
	\end{subtable}
	\caption{Optimized model hyperparameters}
	\label{tab:hyp}
\end{table}




\section{\large{Results}}
The PPO and the A2C agents were trained in the gym-duckietown simulator, with concatenating the last 3, grayscaled observations, and using the \textit{steering} actions and the \textit{orientation} and \textit{velocity} rewards described in \autoref{sec:actions} and \autoref{sec:rewards}. The agents were trained on the \textit{'zigzag\_dists'} map for one million steps, using only 1 environment. Each episode lasted a maximum of 500 steps, because this proved to be enough to take at least a full round on the map for a reasonable agent. 
In order to examine the effects of the hyperparameter optimization, first we trained the models with mostly the default SB3 parameters, the only exception was the learning rate, which was set to a constant 0.0005 value. This was ment to be a baseline training to compare it with the optimized models. The main difference - besides the optimized parameters - was the use of 4 parallel environments and the 2 million steps long training. 
Tables \ref{t:zigzagdists}, \ref{t:smalloop} and \ref{t:udem1} show the evaluation results on the different maps.


\begin{table}[H]
	\caption{Evaluation results on the \textit{'zigzag\_dists'} (seen) map}
	\begin{center}
	\begin{tabular}{|c| c| c| c| c|}
		\hline
	  & PPO (default) & A2C (default) & PPO (optimized) & A2C (optimized) \\
	\hline
	Mean episode reward & 11.51 & 74.24 & 128.19 & 74.24 \\
	\hline
	Mean episode length & 71.5 & 308.0 & 452.9& 308.0 \\
	\hline
	\end{tabular}
	\end{center}
	\label{t:zigzagdists}

	\caption{Evaluation results on the \textit{'loop\_empty'} (unseen) map}
	\begin{center}
	\begin{tabular}{|c| c | c| c | c|}
		\hline
	  & PPO (default) & A2C (default) & PPO (optimized) & A2C (optimized) \\
	\hline
	Mean episode reward & 14.83 & 61.21 & 93.78 & 61.21 \\
	\hline
	Mean episode length & 69.8 & 219.5 & 311.6 & 219.5 \\
	\hline
	\end{tabular}
	\end{center}
	\label{t:smalloop}

	\caption{Evaluation results on the \textit{'udem1'} (unseen) map}
	\begin{center}
	\begin{tabular}{|c| c| c| c| c|}
		\hline
	  & PPO (default) & A2C (default) & PPO (optimized) & A2C (optimized) \\
	\hline
	Mean episode reward & 6.01 & 98.27 & 70.79 & 98.27 \\
	\hline
	Mean episode length & 38.2 & 359.6 & 262.7 & 359.6 \\
	\hline
	\end{tabular}
	\end{center}
	\label{t:udem1}
\end{table}

The evaluation results show us, that in the case of the PPO model, hyperparameter optimization significantly helped achieving higher rewards and longer runs, both in previously seen and unseen environments. 
But regarding the A2C model, the optimization seemingly did not help at all. It is unclear why this has occured, which means further examination of the training and evaluation conditions are needed for the A2C agent.
Another interesting result to consider, is the fact that the A2C model performed better on a map that it has never seen, which is also a more complex version of the \textit{'loop\_empty'} map, with intersections and previously unseen objects like houses and vehicles in the background.
Furthermore, the A2C model achieved much better results with its "default" hyperparameters than the PPO, which could mean it needs less optimization trials. 


\section{\large{Conclusion}}
In this work two reinforcement learning algorithms were trained to drive a two-wheel robot in a simulated environment. 
The models were compared with each other and their optimized versions using basic metrics from the simulation. 
It was found that optimization did help regarding the PPO model, but the A2C agent performed the same way. In order to understand the reasons behind this phenomena more in-depth experiments are needed. 
To further improve the results, longer optimization trials are necessary, with at least 100000 steps, and a variety of maps to train on, which can improve generalization and help avoid overfitting to a single map.
More evaluation methods could be used to elaborate further on the models performance, like transversal and angular deviations from the right lane centerline, or distance travelled in the ego lane.\\


\bibliography{bib}
%\addcontentsline{toc}{chapter}{Irodalomjegyzék}
%\bibliographystyle{plain}
\bibliographystyle{ieeetr}
%\bibliographystyle{unsrt}


\end{document}
