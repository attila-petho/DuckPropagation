### Config file for training

common_config:
  # ID ('base' or 'optimized' for our models, but you can give any unique name for you model)
  ID: '220117_02'
  # Comments
  comment: 'CHANGES: Distance R.: 150->200, NotInLane: -20->-10, Other: lp.dist<-.05: neg.reward, orientation scaling, lin.neg.rew. for angle_deg > 50, LR: C, 5e-5' # Orientation R.: scale:2->3 max_dev_wide:50->30, slope:0.05->0.1, steps: 5e5, Checkpoints: 25k->12.5k, 
  # Seed
  seed: 123
  # Algorithm
  algo: 'PPO'
  # Map name
  map_name: 'zigzag_dists'
  # Number of steps used for training
  steps: '5e5'
  # Number of parallel environments
  n_envs: 4
  # Number of frames to stack
  FS: 3
  # Use color segmentation or grayscale images
  color_segment: False
  # Action wrapper ('heading' or 'leftrightbraking')
  action_wrapper: 'heading'
  # Reward wrapper ('orientation' or 'distance' or 'orientation+distance')
  reward_wrapper: 'orientation+distance'
  # Checkpoint save frequency (divide it by the number of parallel envs)
  checkpoint_freq: 12500
  # Use checkpoints for saving
  checkpoint_cb: True
  # Use domain randomization (1=yes, 0=no)
  domain_rand: 1
  # Learning rate scheduling ('linear' or 'constant')
  lr_schedule: 'constant'
  # Learning rate
  learning_rate: 0.00005
  # The number of steps to run for each environment per update (see SB3 doc.)
  n_steps: 32
  # Factor for trade-off of bias vs variance for Generalized Advantage Estimator
  gae_lambda: 0.92
  # Entropy coefficient for the loss calculation
  ent_coef: 0.0000657
  # Value function coefficient for the loss calculation
  vf_coef: 0.24250834235539484
  # The maximum value for the gradient clipping
  max_grad_norm: 0.8
  # Activation function ('tanh', 'relu', 'elu', 'leaky_relu')
  activation_fn: 'relu'


## PPO only
ppo_config:
  # Minibatch size
  batch_size: 256
  # Clipping parameter
  clip_range: 0.2
  # Number of epoch when optimizing the surrogate loss
  n_epochs: 5


## A2C only
a2c_config:
  # Use RMSprop (default) or Adam as optimizer
  use_rms_prop: False
  # Whether to normalize or not the advantage
  normalize_advantage: False


## Evaluation only
eval_config:
  # Number of evaluation episodes
  n_eval_episodes: 10
  # Plot trajectory of episodes or Visualize camera feed
  plot_trajectory: False
  # Load saved checkpoint
  load_checkpoint: True
  # Checkpoint to load
  checkpoint_step: 100000
  # Save GIF
  save_gif: False

## Paths
#paths:
  
